{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Tutorial - Part 6: Lazy Evaluation\n",
    "\n",
    "In this notebook, we'll explore Polars' lazy evaluation system:\n",
    "- Understanding lazy vs eager execution\n",
    "- Query optimization\n",
    "- Execution plans\n",
    "- Best practices for lazy DataFrames\n",
    "- Advanced lazy operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Eager vs Lazy Execution\n",
    "\n",
    "### 1.1 Eager Execution (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager: Each operation executes immediately\n",
    "df_sales = pl.read_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "print(\"Eager execution:\")\n",
    "result_eager = (\n",
    "    df_sales\n",
    "    .filter(pl.col('revenue') > 500)  # Executes immediately\n",
    "    .group_by('category')  # Executes immediately\n",
    "    .agg([pl.sum('revenue').alias('total_revenue')])  # Executes immediately\n",
    "    .sort('total_revenue', descending=True)  # Executes immediately\n",
    ")\n",
    "\n",
    "print(result_eager)\n",
    "print(f\"Type: {type(result_eager)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lazy Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy: Operations are recorded but not executed\n",
    "lazy_query = (\n",
    "    df_sales.lazy()  # Convert to LazyFrame\n",
    "    .filter(pl.col('revenue') > 500)  # Not executed\n",
    "    .group_by('category')  # Not executed\n",
    "    .agg([pl.sum('revenue').alias('total_revenue')])  # Not executed\n",
    "    .sort('total_revenue', descending=True)  # Not executed\n",
    ")\n",
    "\n",
    "print(\"Lazy query (not executed yet):\")\n",
    "print(f\"Type: {type(lazy_query)}\")\n",
    "print(lazy_query)\n",
    "\n",
    "# Execute the query\n",
    "result_lazy = lazy_query.collect()\n",
    "print(\"\\nAfter .collect():\")\n",
    "print(result_lazy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Optimization\n",
    "\n",
    "### 2.1 Viewing the Execution Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex lazy query\n",
    "complex_query = (\n",
    "    df_sales.lazy()\n",
    "    .filter(pl.col('revenue') > 100)\n",
    "    .select(['date', 'product', 'category', 'revenue', 'region'])\n",
    "    .filter(pl.col('category') == 'Electronics')\n",
    "    .group_by('region')\n",
    "    .agg([\n",
    "        pl.sum('revenue').alias('total_revenue'),\n",
    "        pl.count().alias('num_sales')\n",
    "    ])\n",
    "    .filter(pl.col('num_sales') > 1)\n",
    ")\n",
    "\n",
    "# View the optimized execution plan\n",
    "print(\"Optimized Execution Plan:\")\n",
    "print(complex_query.explain())\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predicate Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars pushes filters down to reduce data early\n",
    "query_with_pushdown = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .select(['product', 'revenue', 'region'])\n",
    "    .filter(pl.col('revenue') > 1000)  # This filter gets pushed down\n",
    ")\n",
    "\n",
    "print(\"Query with Predicate Pushdown:\")\n",
    "print(query_with_pushdown.explain())\n",
    "print(\"\\nNotice how the filter is applied early in the plan!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Projection Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars only reads columns that are actually needed\n",
    "query_projection = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .select(['product', 'revenue'])  # Only these columns are read\n",
    ")\n",
    "\n",
    "print(\"Query with Projection Pushdown:\")\n",
    "print(query_projection.explain())\n",
    "print(\"\\nOnly the required columns are read from the file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lazy Reading Methods\n",
    "\n",
    "### 3.1 scan_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy CSV reading\n",
    "lf_sales = pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "print(\"Lazy CSV scan (no data loaded yet):\")\n",
    "print(type(lf_sales))\n",
    "\n",
    "# Apply operations\n",
    "result = lf_sales.filter(pl.col('category') == 'Furniture').collect()\n",
    "print(\"\\nFiltered results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 scan_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy Parquet reading\n",
    "lf_transactions = pl.scan_parquet(os.path.join(DATA_DIR, 'transactions.parquet'))\n",
    "\n",
    "result = (\n",
    "    lf_transactions\n",
    "    .filter(pl.col('status') == 'completed')\n",
    "    .select(['customer_name', 'amount'])\n",
    "    .limit(5)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Lazy Parquet scan results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison: Eager vs Lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for benchmarking\n",
    "large_df = pl.DataFrame({\n",
    "    'id': range(100000),\n",
    "    'value': [i * 1.5 for i in range(100000)],\n",
    "    'category': ['A', 'B', 'C', 'D'] * 25000,\n",
    "    'subcategory': ['X', 'Y', 'Z'] * 33333 + ['X']\n",
    "})\n",
    "\n",
    "# Save to CSV for lazy reading\n",
    "large_df.write_csv(os.path.join(DATA_DIR, 'large_test.csv'))\n",
    "\n",
    "print(\"Dataset created with 100,000 rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eager execution\n",
    "start = time.time()\n",
    "eager_result = (\n",
    "    pl.read_csv(os.path.join(DATA_DIR, 'large_test.csv'))\n",
    "    .filter(pl.col('value') > 50000)\n",
    "    .filter(pl.col('category').is_in(['A', 'B']))\n",
    "    .select(['id', 'value', 'category'])\n",
    "    .group_by('category')\n",
    "    .agg([pl.count().alias('count')])\n",
    ")\n",
    "eager_time = time.time() - start\n",
    "\n",
    "print(f\"Eager execution time: {eager_time:.4f} seconds\")\n",
    "print(eager_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy execution\n",
    "start = time.time()\n",
    "lazy_result = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'large_test.csv'))\n",
    "    .filter(pl.col('value') > 50000)\n",
    "    .filter(pl.col('category').is_in(['A', 'B']))\n",
    "    .select(['id', 'value', 'category'])\n",
    "    .group_by('category')\n",
    "    .agg([pl.count().alias('count')])\n",
    "    .collect()\n",
    ")\n",
    "lazy_time = time.time() - start\n",
    "\n",
    "print(f\"Lazy execution time: {lazy_time:.4f} seconds\")\n",
    "print(lazy_result)\n",
    "\n",
    "print(f\"\\nSpeedup: {eager_time/lazy_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Lazy Operations\n",
    "\n",
    "### 5.1 Chaining Multiple Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex lazy query with multiple operations\n",
    "advanced_query = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .with_columns([\n",
    "        pl.col('revenue').rank(descending=True).over('category').alias('revenue_rank')\n",
    "    ])\n",
    "    .filter(pl.col('revenue_rank') <= 3)\n",
    "    .sort(['category', 'revenue_rank'])\n",
    "    .select(['category', 'product', 'revenue', 'revenue_rank'])\n",
    ")\n",
    "\n",
    "print(\"Top 3 products per category:\")\n",
    "print(advanced_query.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Using sink_csv() for Lazy Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and write without loading into memory\n",
    "output_file = os.path.join(DATA_DIR, 'processed_output.csv')\n",
    "\n",
    "(\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 1000)\n",
    "    .select(['product', 'category', 'revenue', 'region'])\n",
    "    .sink_csv(output_file)\n",
    ")\n",
    "\n",
    "print(f\"Processed data written to {output_file}\")\n",
    "\n",
    "# Verify\n",
    "verification = pl.read_csv(output_file)\n",
    "print(f\"Rows written: {verification.height}\")\n",
    "print(verification.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Joining Lazy DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regions reference table\n",
    "df_regions = pl.DataFrame({\n",
    "    'region': ['North', 'South', 'East', 'West'],\n",
    "    'region_code': ['N', 'S', 'E', 'W'],\n",
    "    'manager': ['Alice', 'Bob', 'Charlie', 'Diana']\n",
    "})\n",
    "df_regions.write_csv(os.path.join(DATA_DIR, 'regions.csv'))\n",
    "\n",
    "# Lazy join\n",
    "lazy_join = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .join(\n",
    "        pl.scan_csv(os.path.join(DATA_DIR, 'regions.csv')),\n",
    "        on='region',\n",
    "        how='left'\n",
    "    )\n",
    "    .select(['product', 'revenue', 'region', 'manager'])\n",
    "    .filter(pl.col('revenue') > 1000)\n",
    ")\n",
    "\n",
    "print(\"Lazy join execution plan:\")\n",
    "print(lazy_join.explain())\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(lazy_join.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Caching Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .cache() to store intermediate results\n",
    "lazy_cached = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .cache()  # Cache this intermediate result\n",
    ")\n",
    "\n",
    "# Use the cached result multiple times\n",
    "result1 = lazy_cached.group_by('category').agg([pl.sum('revenue')]).collect()\n",
    "result2 = lazy_cached.group_by('region').agg([pl.count()]).collect()\n",
    "\n",
    "print(\"Result 1 (by category):\")\n",
    "print(result1)\n",
    "print(\"\\nResult 2 (by region):\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Execution\n",
    "\n",
    "For datasets larger than memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable streaming for very large datasets\n",
    "streaming_query = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.sum('revenue').alias('total_revenue'),\n",
    "        pl.count().alias('count')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Collect with streaming enabled\n",
    "result = streaming_query.collect(streaming=True)\n",
    "\n",
    "print(\"Streaming execution result:\")\n",
    "print(result)\n",
    "print(\"\\nNote: Streaming processes data in chunks, using constant memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Common Patterns and Best Practices\n",
    "\n",
    "### 9.1 Filter Early, Select Late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Filter before selecting columns\n",
    "good_query = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 1000)  # Filter early\n",
    "    .filter(pl.col('category') == 'Electronics')  # More filtering\n",
    "    .select(['product', 'revenue'])  # Select needed columns\n",
    ")\n",
    "\n",
    "print(\"Optimized query plan:\")\n",
    "print(good_query.explain())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Avoid Collecting Too Early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad: Collecting in the middle of operations\n",
    "# intermediate = pl.scan_csv('file.csv').collect()  # Don't do this\n",
    "# result = intermediate.lazy().filter(...).collect()  # Wasteful\n",
    "\n",
    "# Good: Keep it lazy until the end\n",
    "result = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .group_by('category')\n",
    "    .agg([pl.sum('revenue')])\n",
    "    .collect()  # Only collect at the end\n",
    ")\n",
    "\n",
    "print(\"Efficient lazy execution:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Use with_columns() for Multiple Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient: Multiple columns in one operation\n",
    "efficient_transform = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .with_columns([\n",
    "        (pl.col('revenue') * 1.1).alias('revenue_with_tax'),\n",
    "        (pl.col('price') * 0.9).alias('discounted_price'),\n",
    "        pl.col('product').str.to_uppercase().alias('product_upper')\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Multiple transformations:\")\n",
    "print(efficient_transform.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Debugging Lazy Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .fetch() to preview without full execution\n",
    "lazy_debug = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .select(['product', 'revenue', 'category'])\n",
    ")\n",
    "\n",
    "# Fetch first N rows without optimizing/executing full query\n",
    "preview = lazy_debug.fetch(n_rows=3)\n",
    "\n",
    "print(\"Preview (first 3 rows):\")\n",
    "print(preview)\n",
    "\n",
    "# Use .describe_optimized_plan() for detailed plan\n",
    "print(\"\\nOptimized plan description:\")\n",
    "print(lazy_debug.describe_optimized_plan())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "- ✅ Eager vs lazy execution\n",
    "- ✅ Query optimization techniques\n",
    "- ✅ Predicate and projection pushdown\n",
    "- ✅ Lazy reading methods (scan_csv, scan_parquet)\n",
    "- ✅ Performance comparisons\n",
    "- ✅ Advanced lazy operations\n",
    "- ✅ Streaming execution\n",
    "- ✅ Best practices and common patterns\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Lazy evaluation enables optimization** - Polars can optimize the entire query before execution\n",
    "2. **Use scan_* methods** - For files, always prefer scan_csv/scan_parquet over read_csv/read_parquet\n",
    "3. **Filter early** - Apply filters before aggregations and joins\n",
    "4. **Select only needed columns** - Projection pushdown reduces I/O\n",
    "5. **Collect at the end** - Keep queries lazy as long as possible\n",
    "6. **Use streaming for big data** - Enable streaming=True for datasets larger than memory\n",
    "\n",
    "### When to Use Lazy:\n",
    "- ✅ Reading large files\n",
    "- ✅ Complex multi-step transformations\n",
    "- ✅ When you need optimal performance\n",
    "- ✅ ETL pipelines\n",
    "- ✅ Data larger than available RAM (with streaming)\n",
    "\n",
    "### When to Use Eager:\n",
    "- ✅ Small datasets\n",
    "- ✅ Interactive exploration\n",
    "- ✅ Quick prototyping\n",
    "- ✅ When you need results immediately\n",
    "\n",
    "**Next:** In the final notebook, we'll compare Polars with Pandas!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
