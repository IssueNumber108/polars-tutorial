{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Tutorial - Part 3: Data Manipulation\n",
    "\n",
    "In this notebook, we'll explore advanced data manipulation techniques:\n",
    "- Filtering and selecting with complex conditions\n",
    "- Sorting and ranking\n",
    "- Grouping and aggregations\n",
    "- Joins and merges\n",
    "- Window functions\n",
    "- Data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# Load sample datasets\n",
    "df_sales = pl.read_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "df_employees = pl.read_json(os.path.join(DATA_DIR, 'employees.json'))\n",
    "\n",
    "print(\"Sales Data:\")\n",
    "print(df_sales.head())\n",
    "print(\"\\nEmployee Data:\")\n",
    "print(df_employees.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Filtering\n",
    "\n",
    "### 1.1 Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with multiple AND conditions\n",
    "high_value_electronics = df_sales.filter(\n",
    "    (pl.col('category') == 'Electronics') &\n",
    "    (pl.col('revenue') > 1000)\n",
    ")\n",
    "\n",
    "print(\"High-value electronics sales:\")\n",
    "print(high_value_electronics)\n",
    "\n",
    "# Filter with OR conditions\n",
    "north_or_south = df_sales.filter(\n",
    "    (pl.col('region') == 'North') | (pl.col('region') == 'South')\n",
    ")\n",
    "\n",
    "print(\"\\nNorth or South regions:\")\n",
    "print(north_or_south)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using is_in() for Multiple Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using is_in for multiple values\n",
    "selected_regions = df_sales.filter(\n",
    "    pl.col('region').is_in(['North', 'East'])\n",
    ")\n",
    "\n",
    "print(\"Sales in North and East regions:\")\n",
    "print(selected_regions)\n",
    "\n",
    "# Negate with ~\n",
    "not_electronics = df_sales.filter(\n",
    "    ~pl.col('category').is_in(['Electronics'])\n",
    ")\n",
    "\n",
    "print(\"\\nNon-electronics sales:\")\n",
    "print(not_electronics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 String Pattern Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter using string operations\n",
    "desk_products = df_sales.filter(\n",
    "    pl.col('product').str.contains('Desk')\n",
    ")\n",
    "\n",
    "print(\"Products containing 'Desk':\")\n",
    "print(desk_products)\n",
    "\n",
    "# Case-insensitive matching\n",
    "laptop_products = df_sales.filter(\n",
    "    pl.col('product').str.to_lowercase().str.contains('laptop')\n",
    ")\n",
    "\n",
    "print(\"\\nLaptop products (case-insensitive):\")\n",
    "print(laptop_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Selecting and Transforming Columns\n",
    "\n",
    "### 2.1 Select with Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and transform columns\n",
    "df_transformed = df_sales.select([\n",
    "    pl.col('product'),\n",
    "    pl.col('quantity'),\n",
    "    pl.col('price'),\n",
    "    (pl.col('price') * pl.col('quantity')).alias('calculated_revenue'),\n",
    "    (pl.col('price') * 0.9).alias('discounted_price')\n",
    "])\n",
    "\n",
    "print(\"Transformed columns:\")\n",
    "print(df_transformed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Column Expressions with when-then-otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional column creation\n",
    "df_categorized = df_sales.with_columns([\n",
    "    pl.when(pl.col('revenue') > 2000)\n",
    "      .then(pl.lit('High'))\n",
    "      .when(pl.col('revenue') > 1000)\n",
    "      .then(pl.lit('Medium'))\n",
    "      .otherwise(pl.lit('Low'))\n",
    "      .alias('revenue_category'),\n",
    "    \n",
    "    pl.when(pl.col('quantity') > 10)\n",
    "      .then(pl.col('price') * 0.9)\n",
    "      .otherwise(pl.col('price'))\n",
    "      .alias('bulk_price')\n",
    "])\n",
    "\n",
    "print(\"Categorized data:\")\n",
    "print(df_categorized.select(['product', 'revenue', 'revenue_category', 'quantity', 'bulk_price']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sorting and Ranking\n",
    "\n",
    "### 3.1 Advanced Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns with different orders\n",
    "df_sorted = df_sales.sort(\n",
    "    ['category', 'revenue'],\n",
    "    descending=[False, True]\n",
    ")\n",
    "\n",
    "print(\"Sorted by category (asc) then revenue (desc):\")\n",
    "print(df_sorted.select(['category', 'product', 'revenue']))\n",
    "\n",
    "# Top N items\n",
    "top_5_revenue = df_sales.sort('revenue', descending=True).head(5)\n",
    "print(\"\\nTop 5 by revenue:\")\n",
    "print(top_5_revenue.select(['product', 'revenue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rank column\n",
    "df_ranked = df_sales.with_columns([\n",
    "    pl.col('revenue').rank(method='dense', descending=True).alias('revenue_rank'),\n",
    "    pl.col('quantity').rank(method='ordinal').alias('quantity_rank')\n",
    "])\n",
    "\n",
    "print(\"Data with rankings:\")\n",
    "print(df_ranked.select(['product', 'revenue', 'revenue_rank', 'quantity', 'quantity_rank']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Grouping and Aggregations\n",
    "\n",
    "### 4.1 Basic Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category and aggregate\n",
    "category_stats = df_sales.group_by('category').agg([\n",
    "    pl.count('product').alias('num_transactions'),\n",
    "    pl.sum('revenue').alias('total_revenue'),\n",
    "    pl.mean('revenue').alias('avg_revenue'),\n",
    "    pl.sum('quantity').alias('total_quantity')\n",
    "]).sort('total_revenue', descending=True)\n",
    "\n",
    "print(\"Sales by category:\")\n",
    "print(category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Multiple Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "region_category_stats = df_sales.group_by(['region', 'category']).agg([\n",
    "    pl.sum('revenue').alias('total_revenue'),\n",
    "    pl.mean('price').alias('avg_price'),\n",
    "    pl.count().alias('count')\n",
    "]).sort(['region', 'total_revenue'], descending=[False, True])\n",
    "\n",
    "print(\"Sales by region and category:\")\n",
    "print(region_category_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Advanced Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations on same column\n",
    "product_stats = df_sales.group_by('product').agg([\n",
    "    pl.count().alias('times_sold'),\n",
    "    pl.sum('quantity').alias('total_quantity'),\n",
    "    pl.sum('revenue').alias('total_revenue'),\n",
    "    pl.min('price').alias('min_price'),\n",
    "    pl.max('price').alias('max_price'),\n",
    "    pl.mean('price').alias('avg_price'),\n",
    "    pl.std('price').alias('price_std')\n",
    "]).sort('total_revenue', descending=True)\n",
    "\n",
    "print(\"Detailed product statistics:\")\n",
    "print(product_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Group By with Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter groups after aggregation\n",
    "high_volume_regions = df_sales.group_by('region').agg([\n",
    "    pl.sum('revenue').alias('total_revenue'),\n",
    "    pl.count().alias('num_sales')\n",
    "]).filter(pl.col('num_sales') > 2)\n",
    "\n",
    "print(\"Regions with more than 2 sales:\")\n",
    "print(high_volume_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joining DataFrames\n",
    "\n",
    "### 5.1 Creating a Second DataFrame for Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regions DataFrame\n",
    "df_regions = pl.DataFrame({\n",
    "    'region': ['North', 'South', 'East', 'West'],\n",
    "    'manager': ['Alice Johnson', 'Bob Smith', 'Charlie Davis', 'Diana Martinez'],\n",
    "    'office_city': ['New York', 'Miami', 'Boston', 'Seattle']\n",
    "})\n",
    "\n",
    "print(\"Regions DataFrame:\")\n",
    "print(df_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "df_joined = df_sales.join(df_regions, on='region', how='inner')\n",
    "\n",
    "print(\"Inner join result:\")\n",
    "print(df_joined.select(['product', 'region', 'manager', 'office_city', 'revenue']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join\n",
    "df_left = df_sales.join(df_regions, on='region', how='left')\n",
    "\n",
    "print(\"Left join result:\")\n",
    "print(df_left.select(['product', 'region', 'manager', 'revenue']).head())\n",
    "print(f\"Rows in result: {df_left.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Join with Different Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer DataFrame\n",
    "df_customers = pl.DataFrame({\n",
    "    'cust_id': ['C001', 'C002', 'C003', 'C004', 'C005'],\n",
    "    'customer_name': ['Acme Corp', 'Tech Inc', 'Global Ltd', 'Mega Corp', 'Super Co'],\n",
    "    'industry': ['Manufacturing', 'Technology', 'Finance', 'Retail', 'Healthcare']\n",
    "})\n",
    "\n",
    "# Join on different column names\n",
    "df_with_customers = df_sales.join(\n",
    "    df_customers,\n",
    "    left_on='customer_id',\n",
    "    right_on='cust_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Join with different column names:\")\n",
    "print(df_with_customers.select(['product', 'customer_id', 'customer_name', 'industry', 'revenue']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Window Functions\n",
    "\n",
    "### 6.1 Cumulative Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative sum\n",
    "df_cumsum = df_sales.sort('date').with_columns([\n",
    "    pl.col('revenue').cum_sum().alias('cumulative_revenue'),\n",
    "    pl.col('quantity').cum_sum().alias('cumulative_quantity')\n",
    "])\n",
    "\n",
    "print(\"Cumulative sums:\")\n",
    "print(df_cumsum.select(['date', 'product', 'revenue', 'cumulative_revenue']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Window Functions with over()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate running statistics per category\n",
    "df_window = df_sales.with_columns([\n",
    "    pl.col('revenue').sum().over('category').alias('category_total_revenue'),\n",
    "    pl.col('revenue').mean().over('category').alias('category_avg_revenue'),\n",
    "    pl.col('revenue').max().over('category').alias('category_max_revenue')\n",
    "])\n",
    "\n",
    "print(\"Window functions by category:\")\n",
    "print(df_window.select([\n",
    "    'category', 'product', 'revenue', \n",
    "    'category_total_revenue', 'category_avg_revenue'\n",
    "]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Ranking Within Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank products within each category\n",
    "df_ranked_category = df_sales.with_columns([\n",
    "    pl.col('revenue').rank(method='dense', descending=True).over('category').alias('rank_in_category')\n",
    "])\n",
    "\n",
    "print(\"Ranked within categories:\")\n",
    "print(df_ranked_category.select([\n",
    "    'category', 'product', 'revenue', 'rank_in_category'\n",
    "]).sort(['category', 'rank_in_category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Rolling Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling averages\n",
    "df_rolling = df_sales.sort('date').with_columns([\n",
    "    pl.col('revenue').rolling_mean(window_size=3).alias('revenue_3day_avg'),\n",
    "    pl.col('quantity').rolling_sum(window_size=3).alias('quantity_3day_sum')\n",
    "])\n",
    "\n",
    "print(\"Rolling statistics:\")\n",
    "print(df_rolling.select(['date', 'revenue', 'revenue_3day_avg', 'quantity', 'quantity_3day_sum']).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pivoting and Unpivoting\n",
    "\n",
    "### 7.1 Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "pivot_table = df_sales.pivot(\n",
    "    values='revenue',\n",
    "    index='product',\n",
    "    columns='region',\n",
    "    aggregate_function='sum'\n",
    ")\n",
    "\n",
    "print(\"Pivot table - Revenue by Product and Region:\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Melt (Unpivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wide DataFrame for demonstration\n",
    "df_wide = pl.DataFrame({\n",
    "    'student': ['Alice', 'Bob', 'Charlie'],\n",
    "    'math': [85, 90, 78],\n",
    "    'science': [88, 85, 92],\n",
    "    'english': [90, 88, 85]\n",
    "})\n",
    "\n",
    "print(\"Wide format:\")\n",
    "print(df_wide)\n",
    "\n",
    "# Melt to long format\n",
    "df_long = df_wide.melt(\n",
    "    id_vars='student',\n",
    "    value_vars=['math', 'science', 'english'],\n",
    "    variable_name='subject',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "print(\"\\nLong format:\")\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various string operations\n",
    "df_strings = df_sales.with_columns([\n",
    "    pl.col('product').str.to_uppercase().alias('product_upper'),\n",
    "    pl.col('product').str.to_lowercase().alias('product_lower'),\n",
    "    pl.col('product').str.len_chars().alias('product_length'),\n",
    "    pl.col('region').str.slice(0, 2).alias('region_abbrev')\n",
    "])\n",
    "\n",
    "print(\"String operations:\")\n",
    "print(df_strings.select(['product', 'product_upper', 'product_length', 'region', 'region_abbrev']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Date and Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and extract components\n",
    "df_dates = df_sales.with_columns([\n",
    "    pl.col('date').str.strptime(pl.Date, format='%Y-%m-%d').alias('parsed_date')\n",
    "]).with_columns([\n",
    "    pl.col('parsed_date').dt.year().alias('year'),\n",
    "    pl.col('parsed_date').dt.month().alias('month'),\n",
    "    pl.col('parsed_date').dt.day().alias('day'),\n",
    "    pl.col('parsed_date').dt.weekday().alias('weekday')\n",
    "])\n",
    "\n",
    "print(\"Date operations:\")\n",
    "print(df_dates.select(['date', 'parsed_date', 'year', 'month', 'day', 'weekday']).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "- ✅ Advanced filtering techniques\n",
    "- ✅ Column transformations and conditional logic\n",
    "- ✅ Sorting and ranking\n",
    "- ✅ Grouping and aggregations\n",
    "- ✅ Joining DataFrames\n",
    "- ✅ Window functions\n",
    "- ✅ Pivoting and melting\n",
    "- ✅ String and date operations\n",
    "\n",
    "### Key Takeaways:\n",
    "1. Polars uses expression syntax with `pl.col()` for powerful transformations\n",
    "2. Window functions allow calculations within groups without reducing rows\n",
    "3. The API is designed for method chaining\n",
    "4. All operations are optimized and run in parallel when possible\n",
    "\n",
    "**Next:** In the next notebook, we'll create visualizations from our data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
