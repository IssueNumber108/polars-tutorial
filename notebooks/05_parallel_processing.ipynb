{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Tutorial - Part 5: Parallel Processing\n",
    "\n",
    "In this notebook, we'll explore Polars' parallel processing capabilities:\n",
    "- Understanding automatic parallelization\n",
    "- Multi-threading in Polars\n",
    "- Performance benchmarks\n",
    "- Batch processing\n",
    "- Best practices for parallel operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# Check available CPU cores\n",
    "n_cores = mp.cpu_count()\n",
    "print(f\"Available CPU cores: {n_cores}\")\n",
    "print(f\"Polars version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Automatic Parallelization in Polars\n",
    "\n",
    "Polars automatically parallelizes many operations without any configuration needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for demonstration\n",
    "large_df = pl.DataFrame({\n",
    "    'id': range(1000000),\n",
    "    'value': [i * 2.5 for i in range(1000000)],\n",
    "    'category': ['A', 'B', 'C', 'D'] * 250000,\n",
    "    'random': pl.Series([i % 100 for i in range(1000000)])\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {large_df.shape}\")\n",
    "print(f\"Memory usage: {large_df.estimated_size() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Parallel Filtering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars automatically parallelizes filtering operations\n",
    "start_time = time.time()\n",
    "\n",
    "result = large_df.filter(\n",
    "    (pl.col('value') > 50000) & (pl.col('random') > 50)\n",
    ").select(['id', 'value', 'category'])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Filtered {large_df.height:,} rows to {result.height:,} rows\")\n",
    "print(f\"Time taken: {elapsed:.4f} seconds\")\n",
    "print(f\"\\nFirst few results:\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Parallel Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by operations are automatically parallelized\n",
    "start_time = time.time()\n",
    "\n",
    "aggregated = large_df.group_by('category').agg([\n",
    "    pl.count().alias('count'),\n",
    "    pl.sum('value').alias('total_value'),\n",
    "    pl.mean('value').alias('avg_value'),\n",
    "    pl.std('value').alias('std_value'),\n",
    "    pl.min('value').alias('min_value'),\n",
    "    pl.max('value').alias('max_value')\n",
    "])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Aggregation time: {elapsed:.4f} seconds\")\n",
    "print(f\"\\nAggregated results:\")\n",
    "print(aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Thread Pool Size\n",
    "\n",
    "Polars uses a thread pool for parallel operations. You can control the number of threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current thread pool size\n",
    "print(f\"Default thread pool size: {pl.thread_pool_size()}\")\n",
    "\n",
    "# Note: You can set it with pl.set_thread_pool_size(n)\n",
    "# but it's usually best to let Polars manage this automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Reading of Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple CSV files for parallel reading demonstration\n",
    "for i in range(4):\n",
    "    df_chunk = pl.DataFrame({\n",
    "        'id': range(i*10000, (i+1)*10000),\n",
    "        'value': [x * 1.5 for x in range(i*10000, (i+1)*10000)],\n",
    "        'chunk': [i] * 10000\n",
    "    })\n",
    "    df_chunk.write_csv(os.path.join(DATA_DIR, f'chunk_{i}.csv'))\n",
    "\n",
    "print(\"Created 4 CSV files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process files in parallel using scan_csv\n",
    "import glob\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Using lazy evaluation for parallel processing\n",
    "csv_files = glob.glob(os.path.join(DATA_DIR, 'chunk_*.csv'))\n",
    "\n",
    "# Scan all files and concatenate\n",
    "lazy_frames = [pl.scan_csv(f) for f in csv_files]\n",
    "combined = pl.concat(lazy_frames)\n",
    "\n",
    "# Apply operations and collect\n",
    "result = combined.filter(\n",
    "    pl.col('value') > 5000\n",
    ").select(['id', 'value']).collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Processed {len(csv_files)} files in {elapsed:.4f} seconds\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parallel Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with multiple numeric columns\n",
    "df_multi = pl.DataFrame({\n",
    "    'col1': range(100000),\n",
    "    'col2': range(100000, 200000),\n",
    "    'col3': range(200000, 300000),\n",
    "    'col4': range(300000, 400000)\n",
    "})\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply transformations to multiple columns in parallel\n",
    "result = df_multi.with_columns([\n",
    "    (pl.col('col1') * 2).alias('col1_doubled'),\n",
    "    (pl.col('col2') ** 2).alias('col2_squared'),\n",
    "    (pl.col('col3') / 10).alias('col3_divided'),\n",
    "    (pl.col('col4').log()).alias('col4_log')\n",
    "])\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Parallel column operations time: {elapsed:.4f} seconds\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing with Polars\n",
    "\n",
    "### 5.1 Processing Data in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df_batch, batch_id):\n",
    "    \"\"\"\n",
    "    Process a batch of data\n",
    "    \"\"\"\n",
    "    result = df_batch.with_columns([\n",
    "        (pl.col('value') * 2).alias('value_doubled'),\n",
    "        pl.lit(batch_id).alias('batch_id')\n",
    "    ])\n",
    "    return result\n",
    "\n",
    "# Create sample data\n",
    "full_data = pl.DataFrame({\n",
    "    'id': range(50000),\n",
    "    'value': [i * 1.5 for i in range(50000)]\n",
    "})\n",
    "\n",
    "batch_size = 10000\n",
    "batches = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, full_data.height, batch_size):\n",
    "    batch = full_data.slice(i, batch_size)\n",
    "    processed_batch = process_batch(batch, i // batch_size)\n",
    "    batches.append(processed_batch)\n",
    "\n",
    "# Combine all batches\n",
    "final_result = pl.concat(batches)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Processed {full_data.height:,} rows in {len(batches)} batches\")\n",
    "print(f\"Total time: {elapsed:.4f} seconds\")\n",
    "print(f\"\\nSample output:\")\n",
    "print(final_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison: Operations\n",
    "\n",
    "Let's compare the performance of different operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test DataFrame\n",
    "test_df = pl.DataFrame({\n",
    "    'a': range(500000),\n",
    "    'b': [i * 2 for i in range(500000)],\n",
    "    'c': ['cat_' + str(i % 100) for i in range(500000)]\n",
    "})\n",
    "\n",
    "operations = {}\n",
    "\n",
    "# Test 1: Simple filter\n",
    "start = time.time()\n",
    "_ = test_df.filter(pl.col('a') > 250000)\n",
    "operations['Filter'] = time.time() - start\n",
    "\n",
    "# Test 2: Group by + aggregation\n",
    "start = time.time()\n",
    "_ = test_df.group_by('c').agg([\n",
    "    pl.sum('a').alias('sum_a'),\n",
    "    pl.mean('b').alias('mean_b')\n",
    "])\n",
    "operations['GroupBy + Agg'] = time.time() - start\n",
    "\n",
    "# Test 3: Multiple transformations\n",
    "start = time.time()\n",
    "_ = test_df.with_columns([\n",
    "    (pl.col('a') * 2).alias('a_doubled'),\n",
    "    (pl.col('b') / 10).alias('b_divided'),\n",
    "    pl.col('c').str.to_uppercase().alias('c_upper')\n",
    "])\n",
    "operations['Transformations'] = time.time() - start\n",
    "\n",
    "# Test 4: Sorting\n",
    "start = time.time()\n",
    "_ = test_df.sort(['c', 'a'])\n",
    "operations['Sort'] = time.time() - start\n",
    "\n",
    "# Display results\n",
    "print(\"Performance Benchmark (500,000 rows):\")\n",
    "print(\"=\" * 40)\n",
    "for op, duration in operations.items():\n",
    "    print(f\"{op:20s}: {duration:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lazy Evaluation for Parallel Optimization\n",
    "\n",
    "Lazy evaluation allows Polars to optimize the execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lazy frame\n",
    "df_sales = pl.read_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "# Define lazy operations\n",
    "lazy_query = (\n",
    "    df_sales.lazy()\n",
    "    .filter(pl.col('revenue') > 500)\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.sum('revenue').alias('total_revenue'),\n",
    "        pl.count().alias('count')\n",
    "    ])\n",
    "    .sort('total_revenue', descending=True)\n",
    ")\n",
    "\n",
    "# Show the optimized execution plan\n",
    "print(\"Optimized Execution Plan:\")\n",
    "print(lazy_query.explain())\n",
    "\n",
    "# Execute the query\n",
    "result = lazy_query.collect()\n",
    "print(\"\\nResult:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Streaming Processing\n",
    "\n",
    "For very large datasets that don't fit in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large CSV for streaming demonstration\n",
    "large_csv = os.path.join(DATA_DIR, 'large_data.csv')\n",
    "pl.DataFrame({\n",
    "    'id': range(200000),\n",
    "    'value': [i * 1.5 for i in range(200000)],\n",
    "    'category': ['A', 'B', 'C', 'D'] * 50000\n",
    "}).write_csv(large_csv)\n",
    "\n",
    "# Stream processing with lazy evaluation\n",
    "start_time = time.time()\n",
    "\n",
    "result = (\n",
    "    pl.scan_csv(large_csv)\n",
    "    .filter(pl.col('value') > 10000)\n",
    "    .group_by('category')\n",
    "    .agg([\n",
    "        pl.count().alias('count'),\n",
    "        pl.mean('value').alias('avg_value')\n",
    "    ])\n",
    "    .collect(streaming=True)  # Enable streaming\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Streaming processing time: {elapsed:.4f} seconds\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Parallel Processing\n",
    "\n",
    "### Key Recommendations:\n",
    "\n",
    "1. **Let Polars Handle Parallelization**: Don't manually create threads; Polars optimizes automatically\n",
    "2. **Use Lazy Evaluation**: For complex queries, use `.lazy()` to allow query optimization\n",
    "3. **Batch Processing**: For very large datasets, process in chunks\n",
    "4. **Streaming**: Use `streaming=True` for datasets larger than memory\n",
    "5. **Minimize Data Movement**: Keep data in Polars format as long as possible\n",
    "6. **Use Parquet**: Most efficient format for parallel reading/writing\n",
    "7. **Filter Early**: Apply filters before aggregations to reduce data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of optimal query structure\n",
    "optimal_query = (\n",
    "    pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "    .filter(pl.col('revenue') > 1000)  # Filter early\n",
    "    .select(['product', 'category', 'revenue'])  # Select only needed columns\n",
    "    .group_by(['category', 'product'])\n",
    "    .agg([pl.sum('revenue').alias('total_revenue')])\n",
    "    .sort('total_revenue', descending=True)\n",
    "    .limit(10)  # Limit results if possible\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"Top 10 high-revenue products by category:\")\n",
    "print(optimal_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Monitoring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def monitor_operation(operation_func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Monitor memory and time for an operation\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = operation_func(*args, **kwargs)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    mem_used = mem_after - mem_before\n",
    "    \n",
    "    print(f\"Time: {elapsed:.4f} seconds\")\n",
    "    print(f\"Memory used: {mem_used:.2f} MB\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with a heavy operation\n",
    "print(\"Monitoring heavy aggregation:\")\n",
    "result = monitor_operation(\n",
    "    lambda: large_df.group_by('category').agg([\n",
    "        pl.sum('value'),\n",
    "        pl.mean('value'),\n",
    "        pl.std('value')\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "- ✅ Automatic parallelization in Polars\n",
    "- ✅ Thread pool management\n",
    "- ✅ Parallel file reading and processing\n",
    "- ✅ Batch processing techniques\n",
    "- ✅ Performance benchmarking\n",
    "- ✅ Lazy evaluation and query optimization\n",
    "- ✅ Streaming for large datasets\n",
    "- ✅ Best practices for parallel operations\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Polars parallelizes automatically** - no manual threading needed\n",
    "2. **Lazy evaluation enables optimization** - use `.lazy()` for complex queries\n",
    "3. **Streaming handles large data** - use `streaming=True` when data exceeds memory\n",
    "4. **Filter and select early** - reduce data size before heavy operations\n",
    "5. **Polars is fast by default** - focus on query logic, not parallelization\n",
    "\n",
    "### Performance Tips:\n",
    "- Use Parquet for best I/O performance\n",
    "- Enable streaming for datasets larger than RAM\n",
    "- Let Polars manage thread pool automatically\n",
    "- Chain operations for better optimization\n",
    "\n",
    "**Next:** In the next notebook, we'll dive deeper into lazy evaluation and query optimization!"
   ]
  }
 ],
 "metadata": {
  "kernelnel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
