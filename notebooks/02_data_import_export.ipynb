{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polars Tutorial - Part 2: Data Import and Export\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "- Reading data from various file formats (CSV, JSON, Parquet, Excel)\n",
    "- Writing data to different formats\n",
    "- Handling different encodings and compression\n",
    "- Working with our sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Set data directory path\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading CSV Files\n",
    "\n",
    "### 1.1 Basic CSV Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "df_sales = pl.read_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "print(\"Sales Data:\")\n",
    "print(df_sales)\n",
    "print(f\"\\nShape: {df_sales.shape}\")\n",
    "print(f\"\\nData types:\\n{df_sales.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CSV Reading with Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV with specific options\n",
    "df_sales_typed = pl.read_csv(\n",
    "    os.path.join(DATA_DIR, 'sales_data.csv'),\n",
    "    try_parse_dates=True,  # Automatically parse date columns\n",
    "    null_values=['NA', 'null', ''],  # Treat these as null\n",
    ")\n",
    "\n",
    "print(\"Sales Data with parsed dates:\")\n",
    "print(df_sales_typed)\n",
    "print(f\"\\nDate column type: {df_sales_typed['date'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Reading Specific Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only specific columns\n",
    "df_subset = pl.read_csv(\n",
    "    os.path.join(DATA_DIR, 'sales_data.csv'),\n",
    "    columns=['date', 'product', 'revenue', 'region']\n",
    ")\n",
    "\n",
    "print(\"Subset of columns:\")\n",
    "print(df_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 CSV Scanning (Lazy Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan CSV file lazily (doesn't load into memory immediately)\n",
    "lf_sales = pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "# Apply operations on lazy frame\n",
    "result = lf_sales.filter(\n",
    "    pl.col('revenue') > 1000\n",
    ").select(['product', 'revenue', 'region']).collect()\n",
    "\n",
    "print(\"Lazy reading with filtering:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading JSON Files\n",
    "\n",
    "### 2.1 Basic JSON Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "df_employees = pl.read_json(os.path.join(DATA_DIR, 'employees.json'))\n",
    "\n",
    "print(\"Employee Data:\")\n",
    "print(df_employees)\n",
    "print(f\"\\nShape: {df_employees.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 JSON Lines Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a JSON Lines file for demonstration\n",
    "import json\n",
    "\n",
    "json_lines_data = [\n",
    "    {\"id\": 1, \"name\": \"Item A\", \"value\": 100},\n",
    "    {\"id\": 2, \"name\": \"Item B\", \"value\": 200},\n",
    "    {\"id\": 3, \"name\": \"Item C\", \"value\": 300}\n",
    "]\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'items.jsonl'), 'w') as f:\n",
    "    for item in json_lines_data:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Read JSON Lines\n",
    "df_jsonl = pl.read_ndjson(os.path.join(DATA_DIR, 'items.jsonl'))\n",
    "print(\"JSON Lines data:\")\n",
    "print(df_jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading Parquet Files\n",
    "\n",
    "Parquet is a columnar storage format that's very efficient for analytical workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Parquet file\n",
    "df_transactions = pl.read_parquet(os.path.join(DATA_DIR, 'transactions.parquet'))\n",
    "\n",
    "print(\"Transaction Data:\")\n",
    "print(df_transactions.head(10))\n",
    "print(f\"\\nShape: {df_transactions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Lazy Parquet Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan Parquet file lazily\n",
    "lf_transactions = pl.scan_parquet(os.path.join(DATA_DIR, 'transactions.parquet'))\n",
    "\n",
    "# Query optimization example\n",
    "result = lf_transactions.filter(\n",
    "    pl.col('status') == 'completed'\n",
    ").select(['transaction_id', 'customer_name', 'amount']).limit(5).collect()\n",
    "\n",
    "print(\"Completed transactions (lazy):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Excel file\n",
    "df_students = pl.read_excel(os.path.join(DATA_DIR, 'students.xlsx'))\n",
    "\n",
    "print(\"Student Data:\")\n",
    "print(df_students)\n",
    "print(f\"\\nShape: {df_students.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Reading Specific Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your Excel file has multiple sheets, you can specify which one\n",
    "# df_sheet = pl.read_excel('file.xlsx', sheet_name='Sheet2')\n",
    "\n",
    "# Or read by sheet index (0-indexed)\n",
    "# df_sheet = pl.read_excel('file.xlsx', sheet_id=0)\n",
    "\n",
    "print(\"Excel reading supports sheet selection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing Data to Files\n",
    "\n",
    "### 5.1 Writing to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to CSV\n",
    "output_csv = os.path.join(DATA_DIR, 'output_sales.csv')\n",
    "df_sales.write_csv(output_csv)\n",
    "\n",
    "print(f\"Data written to: {output_csv}\")\n",
    "\n",
    "# Verify by reading back\n",
    "df_verify = pl.read_csv(output_csv)\n",
    "print(f\"Verified - rows written: {df_verify.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Writing to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to JSON\n",
    "output_json = os.path.join(DATA_DIR, 'output_employees.json')\n",
    "df_employees.write_json(output_json)\n",
    "\n",
    "print(f\"Data written to: {output_json}\")\n",
    "\n",
    "# Write to JSON Lines (NDJSON)\n",
    "output_ndjson = os.path.join(DATA_DIR, 'output_employees.ndjson')\n",
    "df_employees.write_ndjson(output_ndjson)\n",
    "\n",
    "print(f\"Data written to: {output_ndjson}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Writing to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet (highly compressed, fast)\n",
    "output_parquet = os.path.join(DATA_DIR, 'output_sales.parquet')\n",
    "df_sales.write_parquet(output_parquet)\n",
    "\n",
    "print(f\"Data written to: {output_parquet}\")\n",
    "\n",
    "# Compare file sizes\n",
    "import os as os_module\n",
    "csv_size = os_module.path.getsize(output_csv)\n",
    "parquet_size = os_module.path.getsize(output_parquet)\n",
    "\n",
    "print(f\"\\nFile size comparison:\")\n",
    "print(f\"CSV: {csv_size:,} bytes\")\n",
    "print(f\"Parquet: {parquet_size:,} bytes\")\n",
    "print(f\"Compression ratio: {csv_size/parquet_size:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Writing to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Excel\n",
    "output_excel = os.path.join(DATA_DIR, 'output_students.xlsx')\n",
    "df_students.write_excel(output_excel)\n",
    "\n",
    "print(f\"Data written to: {output_excel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with Compressed Files\n",
    "\n",
    "### 6.1 Reading Compressed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# Create a gzip compressed CSV\n",
    "csv_gz_path = os.path.join(DATA_DIR, 'sales_data.csv.gz')\n",
    "with open(os.path.join(DATA_DIR, 'sales_data.csv'), 'rb') as f_in:\n",
    "    with gzip.open(csv_gz_path, 'wb') as f_out:\n",
    "        f_out.writelines(f_in)\n",
    "\n",
    "# Polars can read compressed files directly\n",
    "df_compressed = pl.read_csv(csv_gz_path)\n",
    "\n",
    "print(\"Data read from compressed file:\")\n",
    "print(df_compressed.head())\n",
    "print(f\"Rows: {df_compressed.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Writing Compressed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet has built-in compression\n",
    "df_sales.write_parquet(\n",
    "    os.path.join(DATA_DIR, 'sales_compressed.parquet'),\n",
    "    compression='snappy'  # Options: snappy, gzip, lz4, zstd\n",
    ")\n",
    "\n",
    "print(\"Compressed Parquet file created with Snappy compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Reading from URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polars can read directly from URLs\n",
    "# Example with a public dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "\n",
    "try:\n",
    "    df_iris = pl.read_csv(url)\n",
    "    print(\"Iris dataset from URL:\")\n",
    "    print(df_iris.head())\n",
    "    print(f\"Shape: {df_iris.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Reading from URL requires internet connection. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Schema Override and Type Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify schema when reading CSV\n",
    "schema = {\n",
    "    'date': pl.Utf8,  # Read as string first\n",
    "    'product': pl.Utf8,\n",
    "    'category': pl.Utf8,\n",
    "    'quantity': pl.Int64,\n",
    "    'price': pl.Float64,\n",
    "    'revenue': pl.Float64,\n",
    "    'customer_id': pl.Utf8,\n",
    "    'region': pl.Utf8\n",
    "}\n",
    "\n",
    "df_typed = pl.read_csv(\n",
    "    os.path.join(DATA_DIR, 'sales_data.csv'),\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "# Then parse dates\n",
    "df_typed = df_typed.with_columns(\n",
    "    pl.col('date').str.strptime(pl.Date, format='%Y-%m-%d')\n",
    ")\n",
    "\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "print(df_typed.head())\n",
    "print(f\"\\nData types:\\n{df_typed.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple CSV files for demonstration\n",
    "for i in range(3):\n",
    "    df_batch = pl.DataFrame({\n",
    "        'id': range(i*10, (i+1)*10),\n",
    "        'value': [x * 2 for x in range(i*10, (i+1)*10)],\n",
    "        'batch': [i] * 10\n",
    "    })\n",
    "    df_batch.write_csv(os.path.join(DATA_DIR, f'batch_{i}.csv'))\n",
    "\n",
    "# Read and concatenate multiple files\n",
    "import glob\n",
    "\n",
    "batch_files = glob.glob(os.path.join(DATA_DIR, 'batch_*.csv'))\n",
    "dfs = [pl.read_csv(f) for f in batch_files]\n",
    "df_combined = pl.concat(dfs)\n",
    "\n",
    "print(\"Combined data from multiple files:\")\n",
    "print(df_combined)\n",
    "print(f\"Total rows: {df_combined.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Reading Large Files Efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For very large files, use lazy reading and process in chunks\n",
    "lf = pl.scan_csv(os.path.join(DATA_DIR, 'sales_data.csv'))\n",
    "\n",
    "# Show the optimized query plan\n",
    "print(\"Query plan for lazy operations:\")\n",
    "print(lf.filter(pl.col('revenue') > 500).select(['product', 'revenue']).explain())\n",
    "\n",
    "# Execute the query\n",
    "result = lf.filter(pl.col('revenue') > 500).select(['product', 'revenue']).collect()\n",
    "print(\"\\nFiltered results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, we explored:\n",
    "- ✅ Reading CSV, JSON, Parquet, and Excel files\n",
    "- ✅ Writing data to various formats\n",
    "- ✅ Working with compressed files\n",
    "- ✅ Lazy reading for large files\n",
    "- ✅ Reading from URLs\n",
    "- ✅ Schema specification and type control\n",
    "- ✅ Batch processing multiple files\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Parquet** is the most efficient format for large datasets\n",
    "2. **Lazy reading** (`scan_*` methods) is ideal for large files\n",
    "3. Polars handles compression automatically\n",
    "4. Schema specification gives you full control over data types\n",
    "5. Polars can read directly from URLs and compressed files\n",
    "\n",
    "### Performance Tips:\n",
    "- Use Parquet for storage when possible\n",
    "- Use lazy reading for large files\n",
    "- Specify schema to avoid type inference overhead\n",
    "- Read only required columns for better performance\n",
    "\n",
    "**Next:** In the next notebook, we'll dive deep into data manipulation and transformations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
